{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie review Dataset: https://www.kaggle.com/pankrzysiu/keras-imdb <br>\n",
    "Global vectors for word representation: https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5405413f-bb7d-4281-90da-8360e9462e47",
    "_uuid": "4388da78ae3c2fa7202868a88c4ad90f52331235"
   },
   "source": [
    "NLP and Word Embeddings\n",
    "\n",
    "\n",
    "> Natural language processing (NLP) is a field of computer science, artificial intelligence concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language data.\n",
    "\n",
    "> Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n",
    "\n",
    "\n",
    "## Sentiment analysis with the IMDB dataset\n",
    "\n",
    "Sentiment analysis is about judging how positive or negative the tone in a document is. The output of a sentiment analysis is a score between zero and one, where one means the tone is very positive and zero means it is very negative. Sentiment analysis is used for trading quite frequently. For example the sentiment of quarterly reports issued by firms is automatically analyzed to see how the firm judges its own position. Sentiment analysis is also applied to the tweets of traders to estimate an overall market mood. Today, there are many data providers that offer sentiment analysis as a service.\n",
    "\n",
    "In principle, training a sentiment analysis model works just like training a binary text classifier. The text gets classified into positive (1) or not positive (0). This works exactly like other binary classification only that we need some new tools to handle text.\n",
    "\n",
    "A common dataset for sentiment analysis is the corpus of [Internet Movie Database (IMDB)](http://www.imdb.com/) movie reviews. Since each review comes with a text and a numerical rating, the number of stars, it is easy to label the training data. In the IMDB dataset, movie reviews that gave less then five stars where labeled negative while movies that gave more than seven stars where labeled positive (IMDB works with a ten star scale). Let's give the data a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "c0df886c-e951-44db-9505-1b8911b2d198",
    "_uuid": "7265ce457127916d310ca2fe328886479e95f9e4"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "d7d398e2-f868-4264-95f7-cbfb64fda5d0",
    "_uuid": "b9cb3a0376a1cddad5c4746f20ef765f7a77100e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  neg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [01:11<00:00, 173.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [01:24<00:00, 148.50it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "imdb_dir = 'aclImdb_v1/aclImdb' # Data directory\n",
    "train_dir = os.path.join(imdb_dir, 'train') # Get the path of the train set\n",
    "\n",
    "# Setup empty lists to fill\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "# First go through the negatives, then through the positives\n",
    "for label_type in ['neg', 'pos']:\n",
    "    # Get the sub path\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    print('loading ',label_type)\n",
    "    # Loop over all files in path\n",
    "    for fname in tqdm(os.listdir(dir_name)):\n",
    "        \n",
    "        # Only consider text files\n",
    "        if fname[-4:] == '.txt':\n",
    "            # Read the text file and put it in the list\n",
    "            f = open(os.path.join(dir_name, fname), 'rb')\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            # Attach the corresponding label\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b5b0a76d-d4f5-46e9-8425-c606651504f9",
    "_uuid": "d5aee178ecba35acea4ddaec5290952c74a7f4bf"
   },
   "source": [
    "We should have 25,000 texts and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "95c87eb2-5718-4a46-85ba-ade53c733149",
    "_uuid": "6fe7b38cf663f0c05edee2ff996b9103a4fb6541"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "04b76864-e3f8-4e94-8da0-c6de5110272f",
    "_uuid": "9eb9003a200daf1359798889ae4825cac91a47f1"
   },
   "source": [
    "Half of the reviews are positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "33fafe63-b7dd-4c4a-a88f-4ca57af448f7",
    "_uuid": "df130dcdb2a96b04e7276e8835f01c461c5cc17d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "21c3a771-3bca-4315-a3b4-f98fce705d50",
    "_uuid": "93672fc39bf3144dc1f9079a4b405e61f1a01420"
   },
   "source": [
    "Let's look at a positive review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "9155c45f-ac4b-4b8b-a657-f6e5b66be96c",
    "_uuid": "915ca4b43a2d683844156f4e98c2a15333c69fbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 1\n",
      "b\"I saw this movie at the 18th Haifa film festival, and it is one of the best I've seen this year. Seeing it on a big screen (and I mean BIG, not one of those TV screens most cinemas have) with an excellent sound system always enhance the cinematic experience, as the movie takes over your eyes and ears and sucks you into the story, into the picture.<br /><br />The movie presents a set of characters, which are loosely inter-connected. Their stories cross at certain points, and the multiplicity of story lines reminded me very much of the great Robert Altman and his exquisite films. But the true hero of the movie is obviously the city of Madrid, which provides the backdrop for the entire movie. It houses the characters, contains the pavements and roads on which they walk, and sets the background atmosphere for all the events, all in beautifully filmed scenes.<br /><br />The movie returns again and again to certain themes (shoes, for instance), and in essence Salazar makes his metaphores more and more understandable to the viewer as the movie progresses. He combines the views of the city with the shots of the characters, and elegantly matches the feeling of the scene to the background. A set of talented actors helps him portrait a wide variety of characters. One excellent example is the scene in which Juaquin takes Anita across the street for the first time. It might not work on a small screen, but it gave me goose bumps easily on a big screen.<br /><br />The message of the movie is very positive, and accordingly the movie is light and funny at times. The music along the movie is usually pop, with a few instrumental pieces (I hope to put my hand on the soundtrack one day, although I seriously doubt I will).<br /><br />All together, I came out of this movie with a sensational feeling, and I'm not easily impressed (you'll have to take my word for it). For this and more I give this movie a solid 8/10.\"\n"
     ]
    }
   ],
   "source": [
    "print('Label',labels[24002])\n",
    "print(texts[24002])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5608685e-771f-4373-9cee-1341674326c0",
    "_uuid": "d4c3ba90f4dce64356c16ebd2ca89b887476e5fc"
   },
   "source": [
    "And a negative review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "f40de1d3-108e-42fd-b914-acf776b5873e",
    "_uuid": "5984b6c0348b2b3b15bb9991940853eaf5899d8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0\n",
      "b\"Airport '77 starts as a brand new luxury 747 plane is loaded up with valuable paintings & such belonging to rich businessman Philip Stevens (James Stewart) who is flying them & a bunch of VIP's to his estate in preparation of it being opened to the public as a museum, also on board is Stevens daughter Julie (Kathleen Quinlan) & her son. The luxury jetliner takes off as planned but mid-air the plane is hi-jacked by the co-pilot Chambers (Robert Foxworth) & his two accomplice's Banker (Monte Markham) & Wilson (Michael Pataki) who knock the passengers & crew out with sleeping gas, they plan to steal the valuable cargo & land on a disused plane strip on an isolated island but while making his descent Chambers almost hits an oil rig in the Ocean & loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the Bermuda Triangle. With air in short supply, water leaking in & having flown over 200 miles off course the problems mount for the survivor's as they await help with time fast running out...<br /><br />Also known under the slightly different tile Airport 1977 this second sequel to the smash-hit disaster thriller Airport (1970) was directed by Jerry Jameson & while once again like it's predecessors I can't say Airport '77 is any sort of forgotten classic it is entertaining although not necessarily for the right reasons. Out of the three Airport films I have seen so far I actually liked this one the best, just. It has my favourite plot of the three with a nice mid-air hi-jacking & then the crashing (didn't he see the oil rig?) & sinking of the 747 (maybe the makers were trying to cross the original Airport with another popular disaster flick of the period The Poseidon Adventure (1972)) & submerged is where it stays until the end with a stark dilemma facing those trapped inside, either suffocate when the air runs out or drown as the 747 floods or if any of the doors are opened & it's a decent idea that could have made for a great little disaster flick but bad unsympathetic character's, dull dialogue, lethargic set-pieces & a real lack of danger or suspense or tension means this is a missed opportunity. While the rather sluggish plot keeps one entertained for 108 odd minutes not that much happens after the plane sinks & there's not as much urgency as I thought there should have been. Even when the Navy become involved things don't pick up that much with a few shots of huge ships & helicopters flying about but there's just something lacking here. George Kennedy as the jinxed airline worker Joe Patroni is back but only gets a couple of scenes & barely even says anything preferring to just look worried in the background.<br /><br />The home video & theatrical version of Airport '77 run 108 minutes while the US TV versions add an extra hour of footage including a new opening credits sequence, many more scenes with George Kennedy as Patroni, flashbacks to flesh out character's, longer rescue scenes & the discovery or another couple of dead bodies including the navigator. While I would like to see this extra footage I am not sure I could sit through a near three hour cut of Airport '77. As expected the film has dated badly with horrible fashions & interior design choices, I will say no more other than the toy plane model effects aren't great either. Along with the other two Airport sequels this takes pride of place in the Razzie Award's Hall of Shame although I can think of lots of worse films than this so I reckon that's a little harsh. The action scenes are a little dull unfortunately, the pace is slow & not much excitement or tension is generated which is a shame as I reckon this could have been a pretty good film if made properly.<br /><br />The production values are alright if nothing spectacular. The acting isn't great, two time Oscar winner Jack Lemmon has said since it was a mistake to star in this, one time Oscar winner James Stewart looks old & frail, also one time Oscar winner Lee Grant looks drunk while Sir Christopher Lee is given little to do & there are plenty of other familiar faces to look out for too.<br /><br />Airport '77 is the most disaster orientated of the three Airport films so far & I liked the ideas behind it even if they were a bit silly, the production & bland direction doesn't help though & a film about a sunken plane just shouldn't be this boring or lethargic. Followed by The Concorde ... Airport '79 (1979).\"\n"
     ]
    }
   ],
   "source": [
    "print('Label',labels[1])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ae556374-67f9-457c-8d73-76746d1e06ce",
    "_uuid": "10cc5b2ce258e68dd88b1ed38f43c2788a9a5850"
   },
   "source": [
    "## Tokenizing text\n",
    "\n",
    "Computers can not work with words directly. To them, a word is just a meaningless row of characters. To work with words, we need to turn words into so called 'Tokens'. A token is a number that represents that word. Each word gets assigned a token. Tokens are usually assigned by word frequency. The most frequent words like 'a' or 'the' get tokens like 1 or 2 while less often used words like 'profusely' get assigned very high numbers.\n",
    "\n",
    "We can tokenize text directly with Keras. When we tokenize text, we usually choose a maximum number of words we want to consider, our vocabulary so to speak. This prevents us from assigning tokens to words that are hardly ever used, mostly because of typos or because they are not actual words or because they are just very uncommon. This prevents us from over fitting to texts that contain strange words or wired spelling errors. Words that are beyond that cutoff point get assigned the token 0, unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "d566f4e0-b7ce-49a0-a7a5-03da1c570efb",
    "_uuid": "9fec4b6ac16f589362b902661df869f338120935"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "max_words = 10000 # We will only consider the 10K most used words in this dataset\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words) \n",
    "tokenizer.fit_on_texts([x.decode('utf-8') for x in texts]) # Generate tokens by counting frequency\n",
    "sequences = tokenizer.texts_to_sequences([x.decode('utf-8') for x in texts]) # Turn text into sequence of numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "029da5c1-8446-4ca0-964f-07c86c93d000",
    "_uuid": "3f2f317b0cf99969de44dcdbb1323200fbce9292"
   },
   "source": [
    "The tokenizers word index is a dictionary that maps each word to a number. You can see that words that are frequently used in discussions about movies have a lower token number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "c68aea32-7c57-4dac-9155-491f26fcf35d",
    "_uuid": "9ecbdbb4d05a98ad026b7736468168b7e4d4649a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token for \"the\" 1\n",
      "Token for \"Movie\" 17\n",
      "Token for \"generator\" 20385\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Token for \"the\"',word_index['the'])\n",
    "print('Token for \"Movie\"',word_index['movie'])\n",
    "print('Token for \"generator\"',word_index['generator'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ebc140c1-ae36-46c9-8138-6990a90e2a80",
    "_uuid": "7858214885826f926c90a69245e4792aa484cd4a"
   },
   "source": [
    "Our positive review from earlier has now been converted into a sequence of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "6d5dacb7-e1f2-46c6-bc24-626ca2b896a0",
    "_uuid": "c7dcc9b4dc357ff5b2899fe67ae475fb0966070e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 216, 11, 17, 30, 1, 9410, 19, 1413, 2]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 10 words of the sequence tokenized\n",
    "sequences[24002][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8cb29c8d-872d-459f-8b9a-3405ccd12948",
    "_uuid": "bb2e57df71bd1bde03a0bb2553da3bc1d58057ce"
   },
   "source": [
    "To proceed, we now have to make sure that all text sequences we feed into the model have the same length. We can do this with Keras pad sequences tool. It cuts of sequences that are too long and adds zeros to sequences that are too short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "e9c261df-3216-4837-ab13-7daefd1799be",
    "_uuid": "cd59727e37fc394f04ff2b421c98423685c438aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 100)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = 100 # Make all sequences 100 words long\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "print(data.shape) # We have 25K, 100 word sequences now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "db84f940-76f6-4aaa-a436-9ad9648df50d",
    "_uuid": "fe031791c4adf18a40c774d6746d85db316009e7"
   },
   "source": [
    "Now we can turn all data into proper training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "50852a20-7182-4ee7-8eb9-5aa05b86af07",
    "_uuid": "f1351b9c55aa1e8a1a45dabeb17d33ac1b5d2919"
   },
   "outputs": [],
   "source": [
    "labels = np.asarray(labels)\n",
    "\n",
    "# Shuffle data\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "training_samples = 20000  # We will be training on 10K samples\n",
    "validation_samples = 5000  # We will be validating on 10000 samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0a3223f9-bc36-459d-98d0-418a0c0f3f35",
    "_uuid": "399e51108fddc34d4113531ab7fcb920a221d267"
   },
   "source": [
    "## Embeddings\n",
    "\n",
    "words and word tokens are categorical features. As such, we can not directly feed them into the neural net. Just because a word has a larger token value, it does not express a higher value in any way. It is just a different category. Previously, we have dealt with categorical data by turning it into one hot encoded vectors. But for words, this is impractical. Since our vocabulary is 10,000 words, each vector would contain 10,000 numbers which are all zeros except for one. This is highly inefficient. Instead we will use an embedding. \n",
    "\n",
    "Embeddings also turn categorical data into vectors. But instead of creating a one hot vector, we create a vector in which all elements are numbers.\n",
    "\n",
    "![Embedding](https://storage.googleapis.com/aibootcamp/Week%204/assets/embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7bc9aa0c-0f83-4115-97f1-80bc2a6e2d2f",
    "_uuid": "4beca57069afa8206fa9bc2c8f5f4f69c01268be"
   },
   "source": [
    "In practice, embeddings work like a look up table. For each token, they store a vector. When the token is given to the embedding layer, it returns the vector for that token and passes it through the neural network. As the network trains, the embeddings get optimized as well. Remember that neural networks work by calculating the derivative of the loss function with respect to the parameters (weights) of the model. Through backpropagation we can also calculate the derivative of the loss function with respect to the _input_ of the model. Thus we can optimize the embeddings to deliver ideal inputs that help our model. \n",
    "\n",
    "In practice it looks like this: We have to specify how large we want the word vectors to be. A 50 dimensional vector is able to capture good embeddings even for quite large vocabularies. We also have to specify for how many words we want embeddings and how long our sequences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "a8c1976f-979f-4a07-bfbb-553ad2cf7a1b",
    "_uuid": "127d92b3168722446c9369ef016d121bb26a7330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 50)           500000    \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 5000)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                160032    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 660,065\n",
      "Trainable params: 660,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "93f3f705-bb09-474b-85d5-26cdcf0fd496",
    "_uuid": "a94db62797b87fe1e2bd49d9fdacf8b138dcff82"
   },
   "source": [
    "You can see that the embedding layer has 500,000 trainable parameters, that is 50 parameters for each of the 10K words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_cell_guid": "8810f5cb-5733-499c-a53f-a38674e0111d",
    "_uuid": "0b6eb4a4215d0f27f2c4a240c73f169731931a3c"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "ff61f8ab-0c20-4ca1-be28-41e1801496fd",
    "_uuid": "f194161463fdf5cc7599899d1a8a48d7987b717b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.4341 - acc: 0.7884 - val_loss: 0.3431 - val_acc: 0.8552\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 5s 9ms/step - loss: 0.1161 - acc: 0.9611 - val_loss: 0.4865 - val_acc: 0.8218\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 5s 9ms/step - loss: 0.0109 - acc: 0.9983 - val_loss: 0.5617 - val_acc: 0.8362\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.5945 - val_acc: 0.8406\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 5s 9ms/step - loss: 4.5886e-04 - acc: 1.0000 - val_loss: 0.6259 - val_acc: 0.8412\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 5s 9ms/step - loss: 2.5353e-04 - acc: 1.0000 - val_loss: 0.6558 - val_acc: 0.8428\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 5s 9ms/step - loss: 1.5685e-04 - acc: 1.0000 - val_loss: 0.6787 - val_acc: 0.8432\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 1.0013e-04 - acc: 1.0000 - val_loss: 0.7025 - val_acc: 0.8438\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 5s 9ms/step - loss: 6.5620e-05 - acc: 1.0000 - val_loss: 0.7249 - val_acc: 0.8436\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 5s 9ms/step - loss: 4.4430e-05 - acc: 1.0000 - val_loss: 0.7482 - val_acc: 0.8440\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_cell_guid": "f8206ee4-da18-40f9-b3aa-84a1840b8f89",
    "_uuid": "56e8117cc020a0dc9b23d671e7001946d30d5f60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:15, 26351.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "glove_dir = '' # This is the folder with the dataset\n",
    "\n",
    "print('Loading word vectors')\n",
    "embeddings_index = {} # We create a dictionary of word -> embedding\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), 'rb') # Open file\n",
    "\n",
    "# In the dataset, each line represents a new word embedding\n",
    "# The line starts with the word and the embedding values follow\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0] # The first value is the word, the rest are the values of the embedding\n",
    "    embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n",
    "    embeddings_index[word] = embedding # Add embedding to our embedding dictionary\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "33082b7e-0843-4704-a535-b5d10e4029e2",
    "_uuid": "6be566ff8db672626f116ec7a18210729bc2e8b2"
   },
   "source": [
    "Not all words that are in our IMDB vocabulary might be in the GloVe embeddings though. For missing words it is wise to use random embeddings with the same mean and standard deviation as the GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_cell_guid": "984ac64f-63a9-4e9e-8449-1bbfa60db849",
    "_uuid": "491c770c4cc8d609f356e973133aaeda0d002811"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.004451992, 0.4081574)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a matrix of all embeddings\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean = all_embs.mean() # Calculate mean\n",
    "emb_std = all_embs.std() # Calculate standard deviation\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "55015795-aa4d-4079-b1b9-41a4a4b75dc9",
    "_uuid": "4657785ebf4f0647aaa2cd37a61f3261134c4c93"
   },
   "source": [
    "We can now create an embedding matrix holding all word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "6e7a5869-da62-40a4-9370-fd8f3c7b78d5",
    "_uuid": "8c122070147440ef06e7b4f7b7b37d25dd561336"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100 # We now use larger embeddings\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_words, len(word_index)) # How many words are there actually\n",
    "\n",
    "# Create a random matrix with the same mean and std as the embeddings\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_dim))\n",
    "\n",
    "# The vectors need to be in the same position as their index. \n",
    "# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "\n",
    "# Loop over all words in the word index\n",
    "for word, i in word_index.items():\n",
    "    # If we are above the amount of words we want to use we do nothing\n",
    "    if i >= max_words: \n",
    "        continue\n",
    "    # Get the embedding vector for the word\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # If there is an embedding vector, put it in the embedding matrix\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bebeee48-3b2f-4a10-ac65-806ea1709409",
    "_uuid": "6dfaa8ee123a2ab1135f4f32578e2f859c22756f"
   },
   "source": [
    "This embedding matrix can be used as weights for the embedding layer. This way, the embedding layer uses the pre trained GloVe weights instead of random ones. We can also set the embedding layer to not trainable. This means, Keras won't change the weights of the embeddings while training which makes sense since our embeddings are already trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_cell_guid": "87d9825c-5352-49ba-84dd-95856a229983",
    "_uuid": "8eb3810dd047f3d87db597062053cff0ce423c15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 100, 100)          1000000   \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 10000)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                320032    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 320,065\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights = [embedding_matrix], trainable = False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "792a7ea1-94fa-4949-9463-ab5ee6dde74a",
    "_uuid": "f768abd38ac472fe1eb6d3dd466295e773e1569c"
   },
   "source": [
    "Notice that we now have far fewer trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_cell_guid": "94de29b5-8207-4218-ba6a-96d11375b9ce",
    "_uuid": "6b4cd6d30c6a120615bbb916ad29e5cf8a853d0a"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "1bc6ffb0-8c15-4e30-b4d1-f5c86ad608db",
    "_uuid": "2c8c2868c4a449fafb761ae2ffe742cd0e684982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 4s 5ms/step - loss: 0.6702 - acc: 0.5903 - val_loss: 0.6627 - val_acc: 0.6032\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.4302 - acc: 0.8030 - val_loss: 0.7422 - val_acc: 0.6034\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1676 - acc: 0.9385 - val_loss: 0.9975 - val_acc: 0.6086\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0583 - acc: 0.9801 - val_loss: 1.2534 - val_acc: 0.6006\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0256 - acc: 0.9912 - val_loss: 1.4413 - val_acc: 0.6044\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0139 - acc: 0.9948 - val_loss: 1.6062 - val_acc: 0.6034\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0113 - acc: 0.9951 - val_loss: 1.8055 - val_acc: 0.6030\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0179 - acc: 0.9929 - val_loss: 1.9463 - val_acc: 0.5926\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1158 - acc: 0.9574 - val_loss: 1.6724 - val_acc: 0.5968\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0327 - acc: 0.9893 - val_loss: 1.9500 - val_acc: 0.5940\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1e9b00bd-69a1-4006-b63f-45d9f3c64b5a",
    "_uuid": "160fc0abc16966af4e9c33ef584f9e6959166eca"
   },
   "source": [
    "Now our model over fits less but also does worse on the validation set.\n",
    "\n",
    "# Using our model\n",
    "\n",
    "To determine the sentiment of a text, we can now use our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "46adf776-3b24-4e1f-a37c-66aead70a83d",
    "_uuid": "e5528b998159a60d781f0b6ba0e53a6b6b703598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw seq: [[10, 116, 2517, 2517, 23, 1, 115, 33, 23, 1333, 1388, 12, 61, 178, 1, 115, 15, 1708]]\n",
      "padded seq: [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0   10  116\n",
      "  2517 2517   23    1  115   33   23 1333 1388   12   61  178    1  115\n",
      "    15 1708]]\n",
      "positivity: [[0.9561363]]\n"
     ]
    }
   ],
   "source": [
    "# Demo on a positive text\n",
    "my_text = 'I love dogs. Dogs are the best. They are lovely, cuddly animals that only want the best for humans.'\n",
    "\n",
    "seq = tokenizer.texts_to_sequences([my_text])\n",
    "print('raw seq:',seq)\n",
    "seq = pad_sequences(seq, maxlen=maxlen)\n",
    "print('padded seq:',seq)\n",
    "prediction = model.predict(seq)\n",
    "print('positivity:',prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "d424ef2e-6bb7-4475-853e-ec6b0f7c9a4a",
    "_uuid": "54521955be61d1bb8dc3ad4b8b7430f6e6bf17cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw seq: [[1, 3741, 7050, 77, 1145, 108, 389, 80]]\n",
      "padded seq: [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    1 3741 7050   77 1145  108\n",
      "   389   80]]\n",
      "positivity: [[0.01941606]]\n"
     ]
    }
   ],
   "source": [
    "# Demo on a negative text\n",
    "my_text2 = 'The bleak economic outlook will force many small businesses into bankruptcy.'\n",
    "\n",
    "seq = tokenizer.texts_to_sequences([my_text2])\n",
    "print('raw seq:',seq)\n",
    "seq = pad_sequences(seq, maxlen=maxlen)\n",
    "print('padded seq:',seq)\n",
    "prediction = model.predict(seq)\n",
    "print('positivity:',prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "60e5d7ac-5e58-4908-8823-fd484dff075e",
    "_uuid": "70d9ce4504cc22e842ad586332d9c48d2b995174"
   },
   "source": [
    "## Word embeddings as semantic geometry\n",
    "\n",
    "One very interesting aspect of embeddings trained on large numbers of words is that they show patterns in which the geometric relationship between word vectors corresponds to the semantic relationship between these words.\n",
    "\n",
    "![Relations](https://storage.googleapis.com/aibootcamp/Week%204/assets/man_woman.jpg)\n",
    "\n",
    "\n",
    "In the picture above for instance you can see that the direction of feminine words to their male counterparts is roughly the same. In other words, if you where to substract the word vector for 'woman' from the word 'queen' and add the word vector for 'man' you would arrive at 'king'. This also works for other relationships like comparatives and superlatives. \n",
    "\n",
    "![Rel Comp Sup](https://storage.googleapis.com/aibootcamp/Week%204/assets/comparative_superlative.jpg)\n",
    "\n",
    "This highlights some interesting properties of language in which semantic meanings can be seen as directions which can be added or subtracted.\n",
    "\n",
    "A sad side effect of training word vectors on human writing is that it captures human biases. For example it has been [shown](https://www.technologyreview.com/s/602025/how-vector-space-mathematics-reveals-the-hidden-sexism-in-language/) that for word vectors trained on news websites, 'Programmer' - 'Man' + 'Woman' equals 'Homemaker' reflecting the bias in language that assigns the role of homemaker more often to woman than men. Measuring these biases in embeddings and correcting them has become a field of [research on its own](https://www.technologyreview.com/s/602025/how-vector-space-mathematics-reveals-the-hidden-sexism-in-language/) which highlights how even professional writing from news outlets can be biased."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
